# Reinforcement Learning
A general framework for decision-making.
We have an agent with capacity to act, whose decisions are either rewared or punished.
The goal of the agent is to maximize future reward.

__markov decision process__ - mathematical framework when you make decisions with outcomes partialy dependent on you, partialy random.

S - set of states
A - set of actions
R - set of rewards

# Reward
_Sparse reward_ - the actor only gets a reward at the end of a chain of actions (won a round, scored a point)
_Reward shaping_ - custom rule-based reward system

# Value functions
Value function is a prediction of future reward.
How good it is for the actor to be in the state OR take an action in a given state
```
R = r1+ r2+ r3+ r4 +... +rn
```

_Discounted_ value function (care more about immediate reward than distant rewards):
```
R = r1 + yr2 + y^2r3 + y^3r4
```

Intuition is that you have to penalize the entire chain of actions that led to a bad state, not just last frame.

1. State value function
```
vπ(s) = E[G|St=s]
```
_ __State__ value function under policy π = expected return starting at point s and following policy π threreafter_

2. Action value function (Q-function)
```
qπ(s,a) = E[G | St=s, At=a ]
```
_ __Action__ value function under policy π = expected return starting at point s and taking action a_

# Policy  

Defines agent's behavior in different states. Maping from state to an action. A neural net could be a policy.

An optimal policy is such that the expected return is greater than or equal to all other policies π'

vπ(s) > vπ'(s) for all s∈S

```
Π(a|s) = ... # Probablity that given state s, agent will take action a is ...
```
For each state s∈S, Π is a probability distribution over a∈A(s) (all available states). 


Optimal policy has an associated optimal state value function & q function.
```
v*(s) = max_π v(s)
q*(s,a) = max_π qπ(s,a)
```

# Exploration v.s. Exploitation


# RL in autonomous cars
Used for:
• Lateral control
• Longitudinal control
• Lane keeping
• Obstacle avoidance